{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import codecs\n",
    "import re\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pres(fname):\n",
    "    alltxts = []\n",
    "    alllabs = []\n",
    "    s=codecs.open(fname, 'r','utf-8') # pour régler le codage\n",
    "    while True:\n",
    "        txt = s.readline()\n",
    "        if(len(txt))<5:\n",
    "            break\n",
    "        #\n",
    "        lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n",
    "        txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
    "        if lab.count('M') >0:\n",
    "            alllabs.append(-1)\n",
    "        else: \n",
    "            alllabs.append(1)\n",
    "        alltxts.append(txt)\n",
    "    return alltxts,alllabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/AFDpresidentutf8/corpus.tache1.learn.utf8\"\n",
    "alltxts,alllabs = load_pres(path)\n",
    "\n",
    "# # transforme alllabs en 0 ou 1 au lieu de -1 ou 1\n",
    "# alllabs = np.array(alllabs)\n",
    "# alllabs = (1+alllabs)/2\n",
    "# # set to int les labels\n",
    "# alllabs = alllabs.astype(int) \n",
    "# alllabs = list(alllabs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/juldpnt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/juldpnt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import outils de preprocessing\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from functools import partial\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def preprocess(text, remove_punctuation=True, use_stemming=True, remove_stopwords=True):\n",
    "    # En anglais, à priori les accents et autres caractères spéciaux ne sont très utiles\n",
    "    # dans ce contexte, je normalise donc le texte\n",
    "    text = text.lower()\n",
    "    text = unidecode(text)\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    if remove_punctuation:\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models\n",
    "from functools import partial\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pour utiliser le pré-traitement défini plus haut\n",
    "# MAIS avec des paramètres différents\n",
    "keep_stopwords = partial(preprocess, remove_stopwords=False)\n",
    "\n",
    "# Définit une pipeline avec différentes étapes de pré-traitement\n",
    "linearSVC_model = Pipeline(\n",
    "    [\n",
    "        (\"vect\",\n",
    "            CountVectorizer(\n",
    "                preprocessor=keep_stopwords,\n",
    "                ngram_range=(1, 2),),\n",
    "        ),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", LinearSVC()),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si tu veux itérer sur les modèles, il suffit de le faire sur les pipelines. Comme ça pas de prob de paramètres différents comme tout est encapsulé dans le pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille de l'ensemble d'entraînement : 45930\n",
      "Taille de l'ensemble de test : 11483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diviser le dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(alltxts, alllabs, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Taille de l'ensemble d'entraînement :\", len(train_texts))\n",
    "print(\"Taille de l'ensemble de test :\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juldpnt/micromamba/envs/data-science-general/lib/python3.9/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prédiction...\n",
      "Accuracy du modèle : 0.9129147435339197\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Entraînement du modèle...\")\n",
    "linearSVC_model.fit(train_texts, train_labels)\n",
    "\n",
    "print(\"Prédiction...\")\n",
    "predictions = linearSVC_model.predict(test_texts)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "\n",
    "print(\"Accuracy du modèle :\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
