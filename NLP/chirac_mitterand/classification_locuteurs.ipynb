{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification de locuteur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectif : créer une chaine de traitement des données textuelles sur la classification de locuteur\n",
    "\n",
    "#### Jeu de données : citations Chirac Mitterand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyses obligatoires\n",
    "    * Comparer les performances avec différents pré-traitements\n",
    "        * e.g Taille de vocabulaire, unigram/bigram, Stemming, ...\n",
    "    * Implémenter un post-traitement sur les données Chirac/Mitterrand\n",
    "    * Appliquer les traitements optimaux sur les données de test et sauver les résultats dans un fichier txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compléments optionnels\n",
    "    * Analyser les performances avec Word2Vec, en utilisant des stratégies d'agrégation naïves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/AFDpresidentutf8/corpus.tache1.learn.utf8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données Mitterand / Chirac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données:\n",
    "def load_pres(fname):\n",
    "    pattern1 = re.compile(r\"<[0-9]*:[0-9]*:(.)>.*\")\n",
    "    pattern2 = re.compile(r\"<[0-9]*:[0-9]*:.>(.*)\")\n",
    "\n",
    "    with codecs.open(fname, 'r', 'utf-8') as s:\n",
    "        lines = s.readlines()\n",
    "\n",
    "    alltxts = [pattern2.sub(r\"\\1\", txt) for txt in lines if len(txt) >= 5]\n",
    "    alllabs = [-1 if pattern1.sub(r\"\\1\", txt).count('M') > 0 else 1 for txt in lines if len(txt) >= 5]\n",
    "\n",
    "    return alltxts, alllabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltxts,alllabs = load_pres(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification de l'équilibre du plan d'expérience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 86.89669587027329 % de citations attribuées à Mitterand et 13.103304129726718 % de citations attribuées à Chirac \n"
     ]
    }
   ],
   "source": [
    "C, M = np.unique(alllabs, return_counts=True)[1]\n",
    "print(f\"Il y a {M/(C+M) * 100} % de citations attribuées à Mitterand et {C/(C+M) * 100} % de citations attribuées à Chirac \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que le plan d'expérience n'est pas équilibré. Il va falloir échantilloner notre dataset pour se ramener à une situation à l'équilibre (50% des exemples associés à chaque locuteur)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over-sampling dans la classe majoritaire 'Mitterand'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "# ros = RandomOverSampler(sampling_strategy='auto')\n",
    "# alltxts_res, alllabs_res = ros.fit_resample(np.array(alltxts).reshape(-1, 1), alllabs)\n",
    "\n",
    "# ou\n",
    "\n",
    "smote = SMOTE(sampling_strategy='auto')\n",
    "alltxts_res, alllabs_res = smote.fit_resample(np.array(alltxts).reshape(-1, 1), alllabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y a 50.0 % de citations attribuées à Mitterand et 50.0 % de citations attribuées à Chirac \n"
     ]
    }
   ],
   "source": [
    "C, M = np.unique(alllabs_res, return_counts=True)[1]\n",
    "print(f\"Il y a {M/(C+M) * 100} % de citations attribuées à Mitterand et {C/(C+M) * 100} % de citations attribuées à Chirac \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'over sampling a permis le réequilibrage des classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On réassigne les variables pour plus de clarté dans le code\n",
    "alltxts, alllabs = alltxts_res, alllabs_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des mots fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Nécessaire pour la première exécution\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\") \n",
    "\n",
    "# Retire la ponctuation et tokenize\n",
    "translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "tokens = word_tokenize(\" \".join(map(str, alltxts)).translate(translator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtre les stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "freq_dist = FreqDist(tokens)\n",
    "vocab_size = len(freq_dist)\n",
    "print(f\"Taille du vocabulaire : {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcule les 100 bigrammes et trigrams les plus fréquents\n",
    "bigram_freq = FreqDist(ngrams(tokens, 2))\n",
    "trigram_freq = FreqDist(ngrams(tokens, 3))\n",
    "\n",
    "top_100_bigrams = bigram_freq.most_common(100)\n",
    "top_100_trigrams = trigram_freq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "freq_dist.plot(100)\n",
    "plt.savefig('figures/freq_dist.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "bigram_freq.plot(100)\n",
    "plt.savefig('figures/bigram_freq.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "trigram_freq.plot(100)\n",
    "plt.savefig('figures/trigram_freq.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification préalable du meilleur modèle parmi différents modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade de l'analyse, on n'effectue pas encore de prétraitement sur les données. On va juste faire tourner différents modèles afin de trouver le modèle optimal à améliorer ensuite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation du dataset pour l'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sépare le jeu de donneés en ensembles d'apprentissage et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Diviser le dataset\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(alltxts, alllabs, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Taille de l'ensemble d'entraînement :\", len(train_texts))\n",
    "print(\"Taille de l'ensemble de test :\", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# flatten the list of lists\n",
    "train_texts_flat = [text for sublist in train_texts for text in sublist]\n",
    "test_texts_flat = [text for sublist in test_texts for text in sublist]\n",
    "\n",
    "# Convert the text data into vectors\n",
    "train_vectors = vectorizer.fit_transform(train_texts_flat)\n",
    "test_vectors = vectorizer.transform(test_texts_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des différents modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste différents modèles : Naive Bayes, Logistic Regression, XG Boost et un SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Créer le classifieur\n",
    "clf_NB = MultinomialNB()\n",
    "\n",
    "# Entraîner le classifieur\n",
    "clf_NB.fit(train_vectors, train_labels)\n",
    "\n",
    "# Prédire les labels sur l'ensemble de test\n",
    "pred = clf_NB.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Créer le classifieur\n",
    "clf_LR = LogisticRegression()\n",
    "\n",
    "# Entraîner le classifieur\n",
    "clf_LR.fit(train_vectors, train_labels)\n",
    "\n",
    "# Prédire les labels sur l'ensemble de test\n",
    "pred = clf_LR.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Créer le classifieur\n",
    "clf_XGB = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "\n",
    "# Convert list to Series\n",
    "train_labels_series = pd.Series(train_labels)\n",
    "\n",
    "# Map labels from -1,1 to 0,1\n",
    "train_labels_mapped = train_labels_series.map({-1: 0, 1: 1})\n",
    "\n",
    "# Train the classifier with the mapped labels\n",
    "clf_XGB.fit(train_vectors, train_labels_mapped)\n",
    "\n",
    "# Prédire les labels des données de test\n",
    "pred = clf_XGB.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Créer le classifieur\n",
    "clf_SVM = LinearSVC(C=1.0)\n",
    "\n",
    "# Entraîner le classifieur\n",
    "clf_SVM.fit(train_vectors, train_labels)\n",
    "\n",
    "# Prédire les labels sur l'ensemble de test\n",
    "pred = clf_SVM.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des modèles sans prétraitement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "# on va évaluer chaque modèle avec les métriques \"précision\" et \"AUC\"\n",
    "modele = [clf_NB, clf_LR, clf_XGB, clf_SVM]\n",
    "\n",
    "# Initialize a DataFrame to store the results\n",
    "results = pd.DataFrame(columns=['Model', 'Precision', 'AUC'])\n",
    "\n",
    "for model in modele:\n",
    "    model_name = type(model).__name__\n",
    "    \n",
    "    # Calculer la précision\n",
    "    pred = model.predict(test_vectors)\n",
    "    precision = accuracy_score(test_labels, pred)\n",
    "\n",
    "    # Cas particulier pour le modele SVM qui ne possède pas de méthode predict_proba\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        # Calculer l'auc\n",
    "        pred_proba = model.predict_proba(test_vectors)[:, 1]\n",
    "    elif hasattr(model, 'decision_function'):  # utiliser decision_function pour le modele SVM qui n'a pas de méthode predict_proba\n",
    "        pred_proba = model.decision_function(test_vectors)\n",
    "    else:\n",
    "        print(f\"Le modèle {model_name} n'a pas de méthode pour calculer la probabilité\", \"\\n\")\n",
    "        continue\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(test_labels, pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Ajouter les résultats dans un DataFrame qu'on va ensuite afficher pour synthétiser les résultats\n",
    "    results.loc[len(results)] = [model_name, precision, roc_auc]\n",
    "\n",
    "# Afficher les résultats sous forme de graphique\n",
    "results.plot(kind='bar', x='Model', y=['Precision', 'AUC'])\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim([0.4, 1])\n",
    "plt.legend(['Precision', 'AUC'])\n",
    "plt.show()\n",
    "\n",
    "# Afficher le tableau de synthèse des résultats\n",
    "print(results,\"\\n\")\n",
    "\n",
    "max_precision = results['Precision'].max()\n",
    "max_auc = results['AUC'].max()\n",
    "\n",
    "# Indiquer quels sont les modèles qui ont la meilleure précision et le meilleur AUC\n",
    "best_precision_models = results[results['Precision'] == max_precision]['Model'].values\n",
    "best_auc_models = results[results['AUC'] == max_auc]['Model'].values\n",
    "\n",
    "print(f\"Le modèle avec la meilleure précision est {', '.join(best_precision_models)}\")\n",
    "print(f\"Le modèle avec le meilleur AUC est {', '.join(best_auc_models)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe à ce stade que le modèle avec les meilleures performances (selon les métriques évaluées) est le SVM. On va donc poursuivre en cherchant à améliorer ce modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentatives d'amélioration du meilleur modèle en utilisant différents prétraitements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va mettre en place différents prétraitements pour voir si cela peut permettre d'augmenter les performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import outils de preprocessing\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from functools import partial\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "def preprocess(text, remove_punctuation=True, use_stemming=True, remove_stopwords=True):\n",
    "    # En anglais, à priori les accents et autres caractères spéciaux ne sont très utiles\n",
    "    # dans ce contexte, on peut donc normaliser le texte en retirant ces caractères\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = unidecode(text)\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    if remove_punctuation:\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ensuite tester les différentes combinaisons de ces trois pré-traitements pour entraîner le LinearSVC. On espère obtenir un modèle plus performant que sans les prétraitements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(alltxts, alllabs, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Taille de l'ensemble d'entraînement :\", len(train_texts))\n",
    "print(\"Taille de l'ensemble de test :\", len(test_texts))\n",
    "\n",
    "# All possible combinations of the three boolean parameters\n",
    "param_combinations = list(product([True, False], repeat=3))\n",
    "\n",
    "results = []\n",
    "\n",
    "for params in param_combinations:\n",
    "    # Preprocess the text data\n",
    "    train_texts_preprocessed = [preprocess(' '.join(text.flatten()), *params) for text in train_texts]\n",
    "    test_texts_preprocessed = [preprocess(' '.join(text.flatten()), *params) for text in test_texts]\n",
    "\n",
    "    # Reshape the data if necessary\n",
    "    if len(np.array(train_texts_preprocessed).shape) == 1:\n",
    "        train_texts_preprocessed = np.array(train_texts_preprocessed).reshape(-1, 1)\n",
    "    if len(np.array(test_texts_preprocessed).shape) == 1:\n",
    "        test_texts_preprocessed = np.array(test_texts_preprocessed).reshape(-1, 1)\n",
    "\n",
    "    # Train a LinearSVC model\n",
    "    model = LinearSVC()\n",
    "    model.fit(train_texts_preprocessed, train_labels)\n",
    "\n",
    "    # Compute the accuracy and AUC scores\n",
    "    y_pred = model.predict(test_texts_preprocessed)\n",
    "    accuracy = accuracy_score(test_labels, y_pred)\n",
    "    auc = roc_auc_score(test_labels, y_pred)\n",
    "\n",
    "    # Print the scores and the parameter combination\n",
    "    print(f\"Parameter combination: {params}, Accuracy: {accuracy}, AUC: {auc}\")\n",
    "\n",
    "    # Store the scores and the parameter combination\n",
    "    results.append((params, accuracy, auc))\n",
    "\n",
    "# Find the combination of parameters that gives the highest score\n",
    "best_result = max(results, key=lambda x: x[1])\n",
    "print(f\"Best parameter combination: {best_result[0]}, Accuracy: {best_result[1]}, AUC: {best_result[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation des hyperparamètres du modèle le plus performant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(LinearSVC(), param_grid, cv=5)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(train_vectors, train_labels)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best cross-validation score: {best_score}\")\n",
    "\n",
    "# Train and predict with the best parameters\n",
    "clf_SVM = LinearSVC(C=best_params['C'])\n",
    "clf_SVM.fit(train_vectors, train_labels)\n",
    "pred = clf_SVM.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On obtient que le paramètre C optimal est égal à 10. On obtient donc :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# display the accuracy and AUC scores of the best model\n",
    "accuracy = accuracy_score(test_labels, pred)\n",
    "auc = roc_auc_score(test_labels, pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}, AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
